#!/usr/bin/env python3
"""
æµ‹è¯•æ—¥è¯­å¥å­åˆ†æ API

éªŒè¯æ‰€æœ‰åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import requests
import json

API_URL = "http://localhost:8000"

# æµ‹è¯•ç”¨ä¾‹
TEST_CASES = [
    {
        "sentence": "è¡Œã‹ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“",
        "expected_grammar": ["n4_nakereba_narimasen"],
        "description": "æµ‹è¯• N4 è¯­æ³•ï¼šã€œãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ï¼ˆå¿…é¡»ï¼‰"
    },
    {
        "sentence": "å‹‰å¼·ã—ã¦ã„ã¾ã™",
        "expected_grammar": ["n5_teimasu"],
        "description": "æµ‹è¯• N5 è¯­æ³•ï¼šã€œã¦ã„ã¾ã™ï¼ˆè¿›è¡Œæ—¶ï¼‰"
    },
    {
        "sentence": "æ—¥æœ¬ã«è¡ŒããŸã„",
        "expected_grammar": ["n5_tai"],
        "description": "æµ‹è¯• N5 è¯­æ³•ï¼šã€œãŸã„ï¼ˆæƒ³è¦ï¼‰"
    },
]


def test_health():
    """æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    print("=" * 70)
    print("æµ‹è¯• 1: å¥åº·æ£€æŸ¥")
    print("-" * 70)
    
    try:
        response = requests.get(f"{API_URL}/api/v1/health", timeout=5)
        response.raise_for_status()
        data = response.json()
        
        print(f"âœ“ çŠ¶æ€: {data['status']}")
        print(f"âœ“ åˆ†è¯å™¨: {'æ­£å¸¸' if data['components']['tokenizer'] else 'å¤±è´¥'}")
        print(f"âœ“ è¯­æ³•å¼•æ“: {'æ­£å¸¸' if data['components']['grammar_engine'] else 'å¤±è´¥'}")
        print(f"âœ“ è¯æ±‡æ˜ å°„: {'æ­£å¸¸' if data['components']['vocabulary_mapper'] else 'å¤±è´¥'}")
        print(f"âœ“ åˆ†ææœåŠ¡: {'æ­£å¸¸' if data['analysis_service'] else 'å¤±è´¥'}")
        print()
        return True
    except Exception as e:
        print(f"âœ— å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        print()
        return False


def test_analyze(test_case):
    """æµ‹è¯•å¥å­åˆ†æ"""
    print(f"å¥å­: {test_case['sentence']}")
    print(f"è¯´æ˜: {test_case['description']}")
    print()
    
    try:
        response = requests.post(
            f"{API_URL}/api/v1/analyze",
            json={"sentence": test_case['sentence']},
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        response.raise_for_status()
        data = response.json()
        
        # æ˜¾ç¤ºè¯†åˆ«çš„è¯­æ³•
        print(f"âœ“ è¯†åˆ«åˆ° {len(data['grammar_patterns'])} ä¸ªè¯­æ³•æ¨¡å¼:")
        for pattern in data['grammar_patterns']:
            print(f"  â€¢ [{pattern['level']}] {pattern['name']} - {pattern['meaning']}")
            print(f"    åŒ¹é…: {''.join(pattern['structure'])}")
        
        # æ˜¾ç¤ºè¯æ±‡åˆ†æ
        print(f"\nâœ“ è¯æ±‡åˆ†æ ({len(data['tokens'])} ä¸ªè¯):")
        for token in data['tokens']:
            level_str = f"[{token['jlpt_level']}]" if token['jlpt_level'] else "[-]"
            print(f"  â€¢ {token['surface']} ({token['lemma']}) {level_str} - {token['pos']}")
        
        print()
        return True
        
    except Exception as e:
        print(f"âœ— åˆ†æå¤±è´¥: {e}")
        print()
        return False


def main():
    print("=" * 70)
    print("ğŸ§ª æ—¥è¯­å¥å­åˆ†æ API åŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    print()
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    if not test_health():
        print("âš ï¸  æœåŠ¡æœªå¯åŠ¨æˆ–å¼‚å¸¸ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
        print("   å¯åŠ¨å‘½ä»¤: uvicorn app.main:app --reload")
        return
    
    # æµ‹è¯•æ‰€æœ‰å¥å­
    print("=" * 70)
    print("æµ‹è¯• 2: å¥å­åˆ†æ")
    print("=" * 70)
    print()
    
    success_count = 0
    for i, test_case in enumerate(TEST_CASES, 1):
        print(f"æµ‹è¯•ç”¨ä¾‹ {i}/{len(TEST_CASES)}")
        print("-" * 70)
        if test_analyze(test_case):
            success_count += 1
    
    # æ€»ç»“
    print("=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    print(f"âœ“ é€šè¿‡: {success_count}/{len(TEST_CASES)}")
    print(f"âœ— å¤±è´¥: {len(TEST_CASES) - success_count}/{len(TEST_CASES)}")
    print()
    
    if success_count == len(TEST_CASES):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
    print()


if __name__ == "__main__":
    main()

