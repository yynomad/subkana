#!/usr/bin/env python3
"""
ä» GitHub ä¸‹è½½ JLPT æ•°æ®å¹¶è½¬æ¢ä¸ºé¡¹ç›®æ ¼å¼

æ•°æ®æºï¼š
1. Bluskyo/JLPT_Vocabulary - è¯æ±‡æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
2. å…¶ä»– GitHub èµ„æº
"""

import json
import requests
from pathlib import Path
from typing import Dict, List, Optional
import time

# GitHub æ•°æ®æº
BLUSKYO_REPO = "Bluskyo/JLPT_Vocabulary"
BLUSKYO_BASE_URL = "https://raw.githubusercontent.com/Bluskyo/JLPT_Vocabulary/master/data"

# å…¶ä»–å¯èƒ½çš„èµ„æº
JAMSINCLAIR_REPO = "jamsinclair/open-anki-jlpt-decks"


def download_file(url: str, retry: int = 3) -> Optional[dict]:
    """ä¸‹è½½ JSON æ–‡ä»¶"""
    for attempt in range(retry):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            if attempt < retry - 1:
                print(f"âš ï¸  é‡è¯• ({attempt + 1}/{retry})...")
                time.sleep(2)
            else:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
                return None
    return None


def download_bluskyo_vocabulary() -> Dict[str, str]:
    """ä» Bluskyo ä»“åº“ä¸‹è½½æ‰€æœ‰çº§åˆ«çš„è¯æ±‡"""
    print("ğŸ“¥ æ­£åœ¨ä» Bluskyo/JLPT_Vocabulary ä¸‹è½½è¯æ±‡æ•°æ®...")
    print("-" * 60)
    
    all_vocab = {}
    
    for level in ["N5", "N4", "N3", "N2", "N1"]:
        url = f"{BLUSKYO_BASE_URL}/{level}.json"
        print(f"  ä¸‹è½½ {level}...", end=" ")
        
        data = download_file(url)
        if data:
            # Bluskyo æ ¼å¼å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸
            if isinstance(data, list):
                for item in data:
                    # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µå
                    word = (item.get('word') or item.get('kanji') or 
                           item.get('kana') or item.get('vocabulary'))
                    if word:
                        all_vocab[word] = level
            elif isinstance(data, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå¯èƒ½é”®å°±æ˜¯è¯æ±‡
                for word in data.keys():
                    all_vocab[word] = level
            
            count = len(data) if isinstance(data, list) else len(data) if isinstance(data, dict) else 0
            print(f"âœ… {count} è¯")
        else:
            print("âŒ å¤±è´¥")
        
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    return all_vocab


def download_grammar_from_hanabira() -> List[Dict]:
    """å°è¯•ä» Hanabira ä¸‹è½½è¯­æ³•æ•°æ®"""
    print("ğŸ“¥ æ­£åœ¨å°è¯•ä» Hanabira.org ä¸‹è½½è¯­æ³•æ•°æ®...")
    print("-" * 60)
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„ç«¯ç‚¹
    endpoints = [
        "https://hanabira.org/api/grammar",
        "https://www.hanabira.org/api/grammar",
        "https://hanabira.org/downloads/grammar.json",
    ]
    
    for endpoint in endpoints:
        print(f"  å°è¯• {endpoint}...", end=" ")
        data = download_file(endpoint)
        if data:
            print("âœ… æˆåŠŸ")
            return data if isinstance(data, list) else []
        print("âŒ å¤±è´¥")
        time.sleep(1)
    
    return []


def convert_grammar_to_format(raw_grammar: List[Dict]) -> List[Dict]:
    """å°†åŸå§‹è¯­æ³•æ•°æ®è½¬æ¢ä¸ºé¡¹ç›®æ ¼å¼"""
    converted = []
    
    for idx, item in enumerate(raw_grammar):
        # æå–å­—æ®µï¼ˆæ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼‰
        grammar_id = (item.get('id') or item.get('grammar_id') or 
                     f"grammar_{idx}")
        name = (item.get('name') or item.get('pattern') or 
               item.get('grammar_point') or "")
        level = (item.get('level') or item.get('jlpt_level') or "").upper()
        meaning = (item.get('meaning') or item.get('translation') or 
                  item.get('explanation') or "")
        
        # æ„å»º patternï¼ˆéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´ï¼‰
        # è¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€æ¡†æ¶ï¼Œå®é™…éœ€è¦æ ¹æ®æ•°æ®æºè°ƒæ•´
        pattern = item.get('pattern', [])
        if not pattern:
            # å¦‚æœæ²¡æœ‰ç°æˆçš„ patternï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæ„å»º
            structure = item.get('structure', '')
            if structure:
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ ¼å¼è§£æ
                pattern = [{"pos": "å‹•è©"}]  # å ä½ç¬¦
        
        converted.append({
            "id": str(grammar_id),
            "name": str(name),
            "level": str(level),
            "meaning": str(meaning),
            "pattern": pattern if pattern else [{"pos": "å‹•è©"}]
        })
    
    return converted


def merge_vocabulary(existing: Dict[str, str], new: Dict[str, str]) -> Dict[str, str]:
    """åˆå¹¶è¯æ±‡å­—å…¸ï¼ˆæ–°æ•°æ®ä¼˜å…ˆï¼‰"""
    merged = existing.copy()
    merged.update(new)
    return dict(sorted(merged.items()))


def save_json(data: any, filepath: Path):
    """ä¿å­˜ JSON æ–‡ä»¶"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ ä» GitHub ä¸‹è½½ JLPT æ•°æ®")
    print("=" * 60)
    print()
    
    # è¾“å‡ºè·¯å¾„
    vocab_output = Path(__file__).parent.parent / "data" / "vocabulary_levels.json"
    grammar_output = Path(__file__).parent.parent / "data" / "grammar_rules.json"
    
    # 1. ä¸‹è½½è¯æ±‡æ•°æ®
    print("ğŸ“š ä¸‹è½½è¯æ±‡æ•°æ®")
    print("=" * 60)
    
    new_vocab = download_bluskyo_vocabulary()
    
    # åˆå¹¶å·²æœ‰æ•°æ®
    existing_vocab = {}
    if vocab_output.exists():
        try:
            with open(vocab_output, 'r', encoding='utf-8') as f:
                existing_vocab = json.load(f)
        except:
            pass
    
    if new_vocab:
        merged_vocab = merge_vocabulary(existing_vocab, new_vocab)
        save_json(merged_vocab, vocab_output)
        print(f"\nâœ… è¯æ±‡æ•°æ®å·²ä¿å­˜: {len(merged_vocab)} ä¸ªè¯æ±‡")
    else:
        print("\nâš ï¸  æœªä¸‹è½½åˆ°æ–°çš„è¯æ±‡æ•°æ®")
    
    # 2. ä¸‹è½½è¯­æ³•æ•°æ®
    print()
    print("ğŸ“š ä¸‹è½½è¯­æ³•æ•°æ®")
    print("=" * 60)
    
    raw_grammar = download_grammar_from_hanabira()
    
    if raw_grammar:
        converted_grammar = convert_grammar_to_format(raw_grammar)
        save_json(converted_grammar, grammar_output)
        print(f"\nâœ… è¯­æ³•æ•°æ®å·²ä¿å­˜: {len(converted_grammar)} æ¡è§„åˆ™")
    else:
        print("\nâš ï¸  æœªä¸‹è½½åˆ°è¯­æ³•æ•°æ®")
        print("ğŸ’¡ æç¤º: è¯­æ³•æ•°æ®å¯èƒ½éœ€è¦æ‰‹åŠ¨æ•´ç†æˆ–ä»å…¶ä»–æ¥æºè·å–")
    
    # æ‰“å°ç»Ÿè®¡
    print()
    print("=" * 60)
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
    print("=" * 60)
    
    if vocab_output.exists():
        with open(vocab_output, 'r', encoding='utf-8') as f:
            vocab = json.load(f)
        levels = {}
        for level in vocab.values():
            levels[level] = levels.get(level, 0) + 1
        print(f"è¯æ±‡æ€»æ•°: {len(vocab)}")
        for level in ["N5", "N4", "N3", "N2", "N1"]:
            print(f"  {level}: {levels.get(level, 0)} è¯")
    
    if grammar_output.exists():
        with open(grammar_output, 'r', encoding='utf-8') as f:
            grammar = json.load(f)
        levels = {}
        for rule in grammar:
            level = rule.get('level', '')
            levels[level] = levels.get(level, 0) + 1
        print(f"è¯­æ³•è§„åˆ™æ€»æ•°: {len(grammar)}")
        for level in ["N5", "N4", "N3", "N2", "N1"]:
            print(f"  {level}: {levels.get(level, 0)} æ¡")
    
    print()
    print("âœ… å®Œæˆï¼")


if __name__ == "__main__":
    main()

