#!/usr/bin/env python3
"""
ä»å¼€æºèµ„æºä¸‹è½½å¹¶è½¬æ¢ JLPT æ•°æ®

æ”¯æŒçš„æ•°æ®æºï¼š
1. Hanabira.org - å®Œæ•´çš„è¯­æ³•å’Œè¯æ±‡ JSON æ•°æ®é›†ï¼ˆæ¨èï¼‰
2. Bluskyo/JLPT_Vocabulary - GitHub ä»“åº“çš„è¯æ±‡æ•°æ®
3. jamsinclair/open-anki-jlpt-decks - CSV æ ¼å¼çš„è¯æ±‡æ•°æ®
"""

import json
import csv
import requests
from pathlib import Path
from typing import Dict, List, Optional
import time

# æ•°æ®æº URL
HANABIRA_GRAMMAR_URL = "https://hanabira.org/api/grammar"
HANABIRA_VOCAB_URL = "https://hanabira.org/api/vocabulary"
BLUSKYO_BASE_URL = "https://raw.githubusercontent.com/Bluskyo/JLPT_Vocabulary/master/data"
JAMSINCLAIR_BASE_URL = "https://raw.githubusercontent.com/jamsinclair/open-anki-jlpt-decks/master"


def download_from_hanabira_grammar() -> Optional[List[Dict]]:
    """ä» Hanabira.org ä¸‹è½½è¯­æ³•æ•°æ®"""
    print("ğŸ“¥ æ­£åœ¨ä» Hanabira.org ä¸‹è½½è¯­æ³•æ•°æ®...")
    try:
        response = requests.get(HANABIRA_GRAMMAR_URL, timeout=30)
        response.raise_for_status()
        data = response.json()
        print(f"âœ… æˆåŠŸä¸‹è½½ {len(data) if isinstance(data, list) else 'æœªçŸ¥æ•°é‡'} æ¡è¯­æ³•è§„åˆ™")
        return data if isinstance(data, list) else None
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None


def download_from_hanabira_vocab() -> Optional[List[Dict]]:
    """ä» Hanabira.org ä¸‹è½½è¯æ±‡æ•°æ®"""
    print("ğŸ“¥ æ­£åœ¨ä» Hanabira.org ä¸‹è½½è¯æ±‡æ•°æ®...")
    try:
        response = requests.get(HANABIRA_VOCAB_URL, timeout=30)
        response.raise_for_status()
        data = response.json()
        print(f"âœ… æˆåŠŸä¸‹è½½ {len(data) if isinstance(data, list) else 'æœªçŸ¥æ•°é‡'} ä¸ªè¯æ±‡")
        return data if isinstance(data, list) else None
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None


def download_from_bluskyo(level: str) -> Optional[List[Dict]]:
    """ä» Bluskyo/JLPT_Vocabulary ä¸‹è½½è¯æ±‡æ•°æ®"""
    url = f"{BLUSKYO_BASE_URL}/{level}.json"
    print(f"ğŸ“¥ æ­£åœ¨ä» Bluskyo ä¸‹è½½ {level} è¯æ±‡...")
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        data = response.json()
        print(f"âœ… æˆåŠŸä¸‹è½½ {len(data)} ä¸ª {level} è¯æ±‡")
        return data
    except Exception as e:
        print(f"âš ï¸  ä¸‹è½½ {level} å¤±è´¥: {e}")
        return None


def convert_hanabira_grammar_to_format(hanabira_data: List[Dict]) -> List[Dict]:
    """å°† Hanabira è¯­æ³•æ•°æ®è½¬æ¢ä¸ºé¡¹ç›®æ ¼å¼"""
    converted = []
    
    for item in hanabira_data:
        # æå–åŸºæœ¬ä¿¡æ¯
        grammar_id = item.get('id', '')
        name = item.get('name', '')
        level = item.get('level', '').upper()
        meaning = item.get('meaning', '')
        
        # æ„å»º patternï¼ˆéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´ï¼‰
        # è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹è½¬æ¢ï¼Œå®é™…éœ€è¦æ ¹æ® Hanabira çš„æ•°æ®ç»“æ„è°ƒæ•´
        pattern = []
        
        # å¦‚æœæœ‰ç»“æ„ä¿¡æ¯ï¼Œè½¬æ¢ä¸º pattern
        structure = item.get('structure', '')
        if structure:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è§£æ
            # æš‚æ—¶ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            pattern = [
                {"pos": "å‹•è©"}  # å ä½ç¬¦ï¼Œéœ€è¦æ ¹æ®å®é™…æ•°æ®è°ƒæ•´
            ]
        
        converted.append({
            "id": grammar_id or f"hanabira_{len(converted)}",
            "name": name,
            "level": level,
            "meaning": meaning,
            "pattern": pattern
        })
    
    return converted


def convert_hanabira_vocab_to_format(hanabira_data: List[Dict]) -> Dict[str, str]:
    """å°† Hanabira è¯æ±‡æ•°æ®è½¬æ¢ä¸ºé¡¹ç›®æ ¼å¼"""
    vocab_dict = {}
    
    for item in hanabira_data:
        word = item.get('word', '') or item.get('kanji', '') or item.get('kana', '')
        level = item.get('level', '').upper()
        
        if word and level:
            vocab_dict[word] = level
    
    return vocab_dict


def convert_bluskyo_to_format(bluskyo_data: List[Dict], level: str) -> Dict[str, str]:
    """å°† Bluskyo è¯æ±‡æ•°æ®è½¬æ¢ä¸ºé¡¹ç›®æ ¼å¼"""
    vocab_dict = {}
    
    for item in bluskyo_data:
        # Bluskyo æ ¼å¼å¯èƒ½æ˜¯ {"word": "è¡Œã", "reading": "ã„ã", ...}
        word = item.get('word') or item.get('kanji') or item.get('kana')
        if word:
            vocab_dict[word] = level
    
    return vocab_dict


def merge_vocabulary(*vocab_dicts: Dict[str, str]) -> Dict[str, str]:
    """åˆå¹¶å¤šä¸ªè¯æ±‡å­—å…¸ï¼ˆåé¢çš„ä¼˜å…ˆçº§æ›´é«˜ï¼‰"""
    merged = {}
    for vocab_dict in vocab_dicts:
        if vocab_dict:
            merged.update(vocab_dict)
    return dict(sorted(merged.items()))


def save_vocabulary(vocab_dict: Dict[str, str], output_path: str):
    """ä¿å­˜è¯æ±‡æ•°æ®"""
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(vocab_dict, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜ {len(vocab_dict)} ä¸ªè¯æ±‡åˆ° {output_path}")


def save_grammar_rules(rules: List[Dict], output_path: str):
    """ä¿å­˜è¯­æ³•è§„åˆ™"""
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(rules, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜ {len(rules)} æ¡è¯­æ³•è§„åˆ™åˆ° {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ JLPT æ•°æ®ä¸‹è½½å·¥å…·")
    print("=" * 60)
    print()
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    vocab_output = Path(__file__).parent.parent / "data" / "vocabulary_levels.json"
    grammar_output = Path(__file__).parent.parent / "data" / "grammar_rules.json"
    
    # 1. å°è¯•ä» Hanabira.org ä¸‹è½½ï¼ˆæ¨èï¼‰
    print("ğŸ“š æ–¹æ¡ˆ 1: ä» Hanabira.org ä¸‹è½½ï¼ˆæ¨èï¼‰")
    print("-" * 60)
    
    hanabira_vocab = download_from_hanabira_vocab()
    hanabira_grammar = download_from_hanabira_grammar()
    
    if hanabira_vocab:
        vocab_dict = convert_hanabira_vocab_to_format(hanabira_vocab)
        save_vocabulary(vocab_dict, str(vocab_output))
    
    if hanabira_grammar:
        grammar_rules = convert_hanabira_grammar_to_format(hanabira_grammar)
        save_grammar_rules(grammar_rules, str(grammar_output))
    
    # 2. å¦‚æœ Hanabira å¤±è´¥ï¼Œå°è¯•ä» Bluskyo ä¸‹è½½è¯æ±‡
    if not hanabira_vocab:
        print()
        print("ğŸ“š æ–¹æ¡ˆ 2: ä» Bluskyo/JLPT_Vocabulary ä¸‹è½½è¯æ±‡")
        print("-" * 60)
        
        all_vocab = {}
        for level in ["N5", "N4", "N3", "N2", "N1"]:
            bluskyo_data = download_from_bluskyo(level)
            if bluskyo_data:
                level_vocab = convert_bluskyo_to_format(bluskyo_data, level)
                all_vocab.update(level_vocab)
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        if all_vocab:
            # åˆå¹¶å·²æœ‰çš„è¯æ±‡ï¼ˆå¦‚æœæœ‰ï¼‰
            existing_vocab = {}
            if vocab_output.exists():
                try:
                    with open(vocab_output, 'r', encoding='utf-8') as f:
                        existing_vocab = json.load(f)
                except:
                    pass
            
            merged_vocab = merge_vocabulary(existing_vocab, all_vocab)
            save_vocabulary(merged_vocab, str(vocab_output))
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print()
    print("=" * 60)
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
    print("=" * 60)
    
    if vocab_output.exists():
        with open(vocab_output, 'r', encoding='utf-8') as f:
            vocab_data = json.load(f)
        n5 = sum(1 for v in vocab_data.values() if v == "N5")
        n4 = sum(1 for v in vocab_data.values() if v == "N4")
        n3 = sum(1 for v in vocab_data.values() if v == "N3")
        n2 = sum(1 for v in vocab_data.values() if v == "N2")
        n1 = sum(1 for v in vocab_data.values() if v == "N1")
        print(f"è¯æ±‡æ€»æ•°: {len(vocab_data)}")
        print(f"  N5: {n5} è¯")
        print(f"  N4: {n4} è¯")
        print(f"  N3: {n3} è¯")
        print(f"  N2: {n2} è¯")
        print(f"  N1: {n1} è¯")
    
    if grammar_output.exists():
        with open(grammar_output, 'r', encoding='utf-8') as f:
            grammar_data = json.load(f)
        n5 = sum(1 for r in grammar_data if r.get('level') == "N5")
        n4 = sum(1 for r in grammar_data if r.get('level') == "N4")
        n3 = sum(1 for r in grammar_data if r.get('level') == "N3")
        n2 = sum(1 for r in grammar_data if r.get('level') == "N2")
        n1 = sum(1 for r in grammar_data if r.get('level') == "N1")
        print(f"è¯­æ³•è§„åˆ™æ€»æ•°: {len(grammar_data)}")
        print(f"  N5: {n5} æ¡")
        print(f"  N4: {n4} æ¡")
        print(f"  N3: {n3} æ¡")
        print(f"  N2: {n2} æ¡")
        print(f"  N1: {n1} æ¡")
    
    print()
    print("âœ… å®Œæˆï¼")


if __name__ == "__main__":
    main()

